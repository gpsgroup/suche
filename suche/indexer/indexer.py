'''
indexer for suche Search Engine
copyright (c) 2014 Suche
'''
from indexer.models import SucheURL,Link
from indexer.htmlparser import HTMLParser

class Indexer:
    def set_raw(self,raw):
        '''
        sets the raw data row to which the indexer is to operate.
        '''
        self.raw = raw

    def operate(self):
        '''
        operate on the data. This will extract links, etc
        First of all, we have to revert back the changes of previous website data for eg reduce the number 
        of links, reduce word counts, etc. Then, we have to apply the changes due to new data. Finally, move 
        the new data to old data and set oeprated to true
        '''

        parser = HTMLParser(self.raw.new_data, self.raw.url)

        parser.parse()

        #now extract information from the parse and update database
        urls = []

        # we filter out the valid URLs from the list generated by the html parser
        # for example, we want to save only the URLs from certain domains
        # for testing purpose
        
        filteredurls = []
        for url,text in parser.get_links():
            if SucheURL.isvalid(url):
                filteredurls.append((url,text))

        for url,text in filteredurls:
            urls.append(text+"-"+url)
            newurl,created = SucheURL.objects.get_or_create(url = url)
            if created:
                #create the crawl record for the newly generated url
                newdata = CrawlData(url = newurl)
                newdata.save()

        #now go on to create the links
        thisurl = SucheURL.objects.get(url = self.raw.url)
        for url,text in filteredurls:
            desturl = SucheURL.objects.get(url = url)

            if desturl != thisurl:
                link = Link(fromurl = thisurl, tourl = desturl, text = text)
                link.save()
        
        # set the data as operated
        # self.raw.operated = True
        # self.raw.save()
        return '<br/>'.join(urls)

